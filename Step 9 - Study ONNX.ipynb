{"cells":[{"block_group":"259a7e3173e84d099b207fb8c8745b29","cell_type":"code","execution_count":1,"metadata":{"execution_start":1763392910604,"execution_millis":27472,"source_hash":"73c7daa0","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"f91ab85e1db84e93a3368b3943e4ca5e","deepnote_block_group":"259a7e3173e84d099b207fb8c8745b29","deepnote_cell_type":"code","deepnote_sorting_key":"0","deepnote_content_hash":"73c7daa0","deepnote_execution_started_at":"2025-11-17T15:21:50.604Z","deepnote_execution_finished_at":"2025-11-17T15:22:18.076Z","deepnote_source":"!pip install matplotlib openpyxl scikit-learn lightgbm xgboost catboost  --quiet"},"outputs":[{"name":"stdout","text":"\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"source":"!pip install matplotlib openpyxl scikit-learn lightgbm xgboost catboost  --quiet"},{"block_group":"50f7daba90d248e495adb523ecf24248","cell_type":"code","execution_count":2,"metadata":{"execution_start":1763392938124,"execution_millis":0,"source_hash":"c76c7c51","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"caf25eb663a741c482e47e568a45e345","deepnote_block_group":"50f7daba90d248e495adb523ecf24248","deepnote_cell_type":"code","deepnote_sorting_key":"1","deepnote_content_hash":"c76c7c51","deepnote_execution_started_at":"2025-11-17T15:22:18.124Z","deepnote_execution_finished_at":"2025-11-17T15:22:18.124Z","deepnote_source":"import pandas as pd\nimport numpy as np"},"outputs":[],"source":"import pandas as pd\nimport numpy as np"},{"block_group":"23d0a0f3b18a4645841f3c7473f75e80","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"cd8517a3cb69409b9e6fe3c403c7d44e","deepnote_block_group":"23d0a0f3b18a4645841f3c7473f75e80","deepnote_cell_type":"markdown","deepnote_sorting_key":"2","deepnote_source":"## **Brief Overview about ONNX**"},"source":"## **Brief Overview about ONNX**"},{"block_group":"6d1af679a2c543b9a271fe070bda4388","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"0256721cae254cf08fad846c8a396c80","deepnote_block_group":"6d1af679a2c543b9a271fe070bda4388","deepnote_cell_type":"markdown","deepnote_sorting_key":"3","deepnote_source":"The application of deep learning models in products is equally important and challenging, alongside the research into more accurate and faster deep learning models. A major difficulty lies in converting a model from one framework to another, as each library uses distinct functions and data types.\n\nFor example, when conducting research and experimentation, researchers often use PyTorch because of its ease of use and its popularity within the research community, which facilitates reference and collaboration. Yet, for product deployment, certain tools may exclusively support TensorFlow, necessitating the conversion of models from PyTorch to TensorFlow.\n\nTo address this, we require a standard data format for both the functions and the data types involved in the conversion. ONNX (Open Neural Network Exchange) is the key that can solve all these issues."},"source":"The application of deep learning models in products is equally important and challenging, alongside the research into more accurate and faster deep learning models. A major difficulty lies in converting a model from one framework to another, as each library uses distinct functions and data types.\n\nFor example, when conducting research and experimentation, researchers often use PyTorch because of its ease of use and its popularity within the research community, which facilitates reference and collaboration. Yet, for product deployment, certain tools may exclusively support TensorFlow, necessitating the conversion of models from PyTorch to TensorFlow.\n\nTo address this, we require a standard data format for both the functions and the data types involved in the conversion. ONNX (Open Neural Network Exchange) is the key that can solve all these issues."},{"block_group":"5d68c7dd82b04ebab56ab31810da7be7","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"0049fb880bef49459bb17ae499dfe561","deepnote_block_group":"5d68c7dd82b04ebab56ab31810da7be7","deepnote_cell_type":"markdown","deepnote_sorting_key":"4","deepnote_source":"**ONNX**, which stands for **Open Neural Network Exchange**, serves as an intermediary tool that facilitates the conversion of machine learning models from various frameworks into a standard format provided by ONNX, thereby enabling easy transitions between different frameworks. ONNX supports conversions among many popular frameworks today, including Keras, TensorFlow, Scikit-learn, PyTorch, and XGBoost.\n\n### ONNX's Mechanism: The Key to Interoperability\n\nONNX achieves this seamless conversion through three core elements:\n\n1.  **Standardized Graph Representation:** Since each framework has its own unique computational graph representation, ONNX provides a **standard graph**. This graph is expressed using multiple computational **nodes (operators)** that can represent the graphs of all supported frameworks.\n2.  **Standardized Data Types:** ONNX offers standard data types, such as $\\text{int}8$, $\\text{int}16$, $\\text{float}16$, and others.\n3.  **Standardized Operators (Functions):** ONNX provides a set of standard functions (operators) that can be mapped to their corresponding functions in the target framework. For example, the softmax function in PyTorch will be converted to the corresponding softmax operator in ONNX.\n\n### ONNX Conversion Types\n\nONNX supports two primary conversion methods:\n\n1.  **Trace-based:** This method involves providing an input to the model and then executing the model. The operators (functions) used by the model during this execution process are recorded (traced). A crucial point to note is that if your model is a **dynamic model** (e.g., using different functions based on the input data), the converted model may be inaccurate.\n2.  **Script-based:** In this method, the model is exported as a **ScriptModule**."},"source":"**ONNX**, which stands for **Open Neural Network Exchange**, serves as an intermediary tool that facilitates the conversion of machine learning models from various frameworks into a standard format provided by ONNX, thereby enabling easy transitions between different frameworks. ONNX supports conversions among many popular frameworks today, including Keras, TensorFlow, Scikit-learn, PyTorch, and XGBoost.\n\n### ONNX's Mechanism: The Key to Interoperability\n\nONNX achieves this seamless conversion through three core elements:\n\n1.  **Standardized Graph Representation:** Since each framework has its own unique computational graph representation, ONNX provides a **standard graph**. This graph is expressed using multiple computational **nodes (operators)** that can represent the graphs of all supported frameworks.\n2.  **Standardized Data Types:** ONNX offers standard data types, such as $\\text{int}8$, $\\text{int}16$, $\\text{float}16$, and others.\n3.  **Standardized Operators (Functions):** ONNX provides a set of standard functions (operators) that can be mapped to their corresponding functions in the target framework. For example, the softmax function in PyTorch will be converted to the corresponding softmax operator in ONNX.\n\n### ONNX Conversion Types\n\nONNX supports two primary conversion methods:\n\n1.  **Trace-based:** This method involves providing an input to the model and then executing the model. The operators (functions) used by the model during this execution process are recorded (traced). A crucial point to note is that if your model is a **dynamic model** (e.g., using different functions based on the input data), the converted model may be inaccurate.\n2.  **Script-based:** In this method, the model is exported as a **ScriptModule**."},{"block_group":"c0e2273272684cb0997b3d6b099dd367","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"96dd231cc43e4ea886ed42a26bf87914","deepnote_block_group":"c0e2273272684cb0997b3d6b099dd367","deepnote_cell_type":"markdown","deepnote_sorting_key":"5","deepnote_source":"\n## **When should you use ONNX?**\n\nYou should consider converting your model to ONNX if:\n\n1. **You need to deploy the model in a cleaner production environment**\n.Let's say you have a more complex scenario with few neural networks and some of them are trained using PyTorch while others are trained using TensorFlow. In this case, if you use ONNX , you do not need to install specific versions of PyTorch or TensorFlow when deploying your models as you only need on ONNX Runtime so it becomes way easier to combine multiple models.\nONNX is especially useful when:\n* Running on high-performance servers\n* Running on edge devices (IoT sensors, gateways)\n* Deploying in a mobile app\n* Or if the system requires real-time predictions\n\n2. **Performance Gains**\n  ONNX Runtime can run faster than other frameworks such as:\n* 20‚Äì50% faster than TensorFlow\n* 10‚Äì40% faster than PyTorch\n  ‚Üí Especially useful for time-series models such as LSTM, TCN, or Transformer.\n\n3. **Using specific backends providers with ONNX runtime**\n   This let you leverage hardware accelerators such as: NVIDIA TensorRT, Intel OpenVINO, or AMD MIOpen to speed up inference. Also, training in Python but deploying on a C#/C++/Java application means that you can switch back and forth between multiple programming languages.\n### **When is ONNX not necessary?**\n\nYou don‚Äôt need ONNX if:\n\n1. **You are only analyzing data in a notebook and not deploying**\n\n* Training experimentally\n* Running locally\n* Only doing EDA or prototyping\n  ‚Üí PyTorch or TensorFlow alone is sufficient.\n\n2. **The model uses many custom operators**\nFor example:\n\n* Custom PyTorch modules\n* Custom TensorFlow layers\n  ‚Üí Converting to ONNX may be difficult or not fully supported.\n\n**Conclusion**\n\nIf you want to deploy a model for real-world use, **ONNX is recommended** for faster inference and easier deployment.\nIf you are only analyzing or experimenting in a notebook, **ONNX is not necessary**.\n"},"source":"\n## **When should you use ONNX?**\n\nYou should consider converting your model to ONNX if:\n\n1. **You need to deploy the model in a cleaner production environment**\n.Let's say you have a more complex scenario with few neural networks and some of them are trained using PyTorch while others are trained using TensorFlow. In this case, if you use ONNX , you do not need to install specific versions of PyTorch or TensorFlow when deploying your models as you only need on ONNX Runtime so it becomes way easier to combine multiple models.\nONNX is especially useful when:\n* Running on high-performance servers\n* Running on edge devices (IoT sensors, gateways)\n* Deploying in a mobile app\n* Or if the system requires real-time predictions\n\n2. **Performance Gains**\n  ONNX Runtime can run faster than other frameworks such as:\n* 20‚Äì50% faster than TensorFlow\n* 10‚Äì40% faster than PyTorch\n  ‚Üí Especially useful for time-series models such as LSTM, TCN, or Transformer.\n\n3. **Using specific backends providers with ONNX runtime**\n   This let you leverage hardware accelerators such as: NVIDIA TensorRT, Intel OpenVINO, or AMD MIOpen to speed up inference. Also, training in Python but deploying on a C#/C++/Java application means that you can switch back and forth between multiple programming languages.\n### **When is ONNX not necessary?**\n\nYou don‚Äôt need ONNX if:\n\n1. **You are only analyzing data in a notebook and not deploying**\n\n* Training experimentally\n* Running locally\n* Only doing EDA or prototyping\n  ‚Üí PyTorch or TensorFlow alone is sufficient.\n\n2. **The model uses many custom operators**\nFor example:\n\n* Custom PyTorch modules\n* Custom TensorFlow layers\n  ‚Üí Converting to ONNX may be difficult or not fully supported.\n\n**Conclusion**\n\nIf you want to deploy a model for real-world use, **ONNX is recommended** for faster inference and easier deployment.\nIf you are only analyzing or experimenting in a notebook, **ONNX is not necessary**.\n"},{"block_group":"97f02f4626514b739902eb08526bd6fd","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"bccf32e59d9043db93477882d3d87f03","deepnote_block_group":"97f02f4626514b739902eb08526bd6fd","deepnote_cell_type":"markdown","deepnote_sorting_key":"6","deepnote_source":"## Appplication to this project"},"source":"## Appplication to this project"},{"block_group":"dffc6f7e402c4afd9075e4e803e323a4","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"2b4a99b12bb14fde8a3d76d4559b9e18","deepnote_block_group":"dffc6f7e402c4afd9075e4e803e323a4","deepnote_cell_type":"markdown","deepnote_sorting_key":"7","deepnote_source":""},"source":""},{"block_group":"d2aafe90c66c4dc7a16bacec354acd91","cell_type":"code","execution_count":3,"metadata":{"execution_start":1763392938174,"execution_millis":1928,"source_hash":"3f298931","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"df59232a83ca498d97a26df28c839531","deepnote_block_group":"d2aafe90c66c4dc7a16bacec354acd91","deepnote_cell_type":"code","deepnote_sorting_key":"8","deepnote_content_hash":"3f298931","deepnote_execution_started_at":"2025-11-17T15:22:18.174Z","deepnote_execution_finished_at":"2025-11-17T15:22:20.102Z","deepnote_source":"df = pd.read_excel(\"HCMWeatherDaily.xlsx\")\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\ndf = df.sort_values(\"datetime\")"},"outputs":[],"source":"df = pd.read_excel(\"HCMWeatherDaily.xlsx\")\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\ndf = df.sort_values(\"datetime\")"},{"block_group":"d91ab62bca81438faf6a79f51d212ce5","cell_type":"code","execution_count":4,"metadata":{"execution_start":1763392940164,"execution_millis":0,"source_hash":"6f1f41f7","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"e3270535719e4131aed425f3c38d5ef7","deepnote_block_group":"d91ab62bca81438faf6a79f51d212ce5","deepnote_cell_type":"code","deepnote_sorting_key":"9","deepnote_content_hash":"6f1f41f7","deepnote_execution_started_at":"2025-11-17T15:22:20.164Z","deepnote_execution_finished_at":"2025-11-17T15:22:20.164Z","deepnote_source":"def feature_eng(df):\n        df = df.copy().sort_values(by = ['datetime'])\n        #drop uneeded data\n        columns_to_drop = [\n            'name', 'snow', 'snowdepth', 'name', 'stations', 'conditions','description', 'severerisk', 'sunset', 'sunrise', 'moonphase', 'precipprob', 'uvindex'\n        ]\n        df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n\n        #create time feature\n        df_cleaned['month'] = df_cleaned['datetime'].dt.month\n        df_cleaned['dfy'] = df_cleaned['datetime'].dt.dayofyear\n        # df_cleaned['year'] = df_cleaned['datetime'].dt.year\n        # make sense the characteristics\n        df_cleaned['month_sin'] = np.sin(2 * np.pi * df_cleaned['month'] / 12)\n        df_cleaned['month_cos'] = np.cos(2 * np.pi * df_cleaned['month'] / 12)\n        df_cleaned['dfy_sin'] = np.sin(2 * np.pi * df_cleaned['dfy'] / 365)\n        df_cleaned['dfy_cos'] = np.cos(2 * np.pi * df_cleaned['dfy'] / 365)\n\n        time_df = df_cleaned[['month_sin', 'month_cos', 'dfy_sin', 'dfy_cos']]\n        \n        # ROLLING FEATURES\n        df_cleaned['winddir_sin'] = np.sin(np.deg2rad(df_cleaned['winddir']))\n        df_cleaned['winddir_cos'] = np.cos(np.deg2rad(df_cleaned['winddir']))\n        rolling_fea = ['winddir_cos', 'winddir_sin', 'dew', 'humidity', 'precip', 'precipcover', 'visibility', 'solarenergy', 'cloudcover', 'windspeed']\n        \n        # DERIVED FEATURES\n        derived = {}\n        derived['temp_range_lag1'] = df_cleaned['tempmax'].shift(1) - df_cleaned['tempmin'].shift(1)\n        derived['dew_temp_diff'] = df_cleaned['dew'].shift(1) - df_cleaned['temp'].shift(1)\n        derived['solar_per_cloud'] = df_cleaned['solarenergy'].shift(1) * (1- df_cleaned['cloudcover'].shift(1))/100\n        derived['humid_rad_ratio'] = df_cleaned['humidity'].shift(1)/ (df_cleaned['solarradiation'].shift(1)+1e-6)\n        derived['wind_humidity_interaction'] = df_cleaned['humidity'].shift(1) * (df_cleaned['windspeed'].shift(1)) / 100\n        derived['temp_humid'] = df_cleaned['temp'].shift(1) * df_cleaned['humidity'].shift(1)\n        derived['heat_index'] = df_cleaned['feelslike'].shift(1) - df_cleaned['temp'].shift(1)\n        derived['flmax_humid'] = df_cleaned['feelslikemax'].shift(1) * df_cleaned['humidity'].shift(1)/100\n        derived['flmin_cloud'] = df_cleaned['feelslikemin'].shift(1) * df_cleaned['cloudcover'].shift(1)/100\n        derived['sea_level_pressure_tendency'] = df_cleaned['sealevelpressure'].shift(1) - df_cleaned['sealevelpressure'].shift(6)\n        #df_cleaned['wind_dir'].head(10)\n        # interaction of solar energy and cloud cover\n        derived_df = pd.DataFrame(derived)\n        df_cleaned = pd.concat([df_cleaned, derived_df], axis=1)\n        \n        \n        \n        #onehot encode\n        nominal_cols = ['icon','preciptype']\n        df_encoded = pd.get_dummies(df_cleaned[nominal_cols], drop_first = True).astype(int)\n        df_no_cat = df_cleaned.drop(columns = nominal_cols, errors = 'ignore')\n        # Combine encoded features\n        df_cleaned = pd.concat([df_no_cat, df_encoded], axis = 1)\n        # print(df_cleaned.columns)\n        \n        # Season features\n        season = {}\n        for feature in ['humidity', 'dew', 'precip', 'windspeed']:\n            season[f'{feature}_seasonal'] = df_cleaned[feature].shift(1).rolling(3).max() - df_cleaned[feature].shift(1).rolling(7).max()\n            season[f'{feature}_trend'] = df_cleaned[feature].shift(1) - df_cleaned[feature].shift(2)\n            season[f'{feature}_derivative'] = season[f'{feature}_trend'].shift(1) - season [f'{feature}_trend'].shift(2)\n\n        season_df = pd.DataFrame(season)\n        df_cleaned = pd.concat([df_cleaned, season_df], axis = 1)\n        \n        rolling_columns = {}\n        for num in [7, 21, 42, 84, 126, 182]:\n            for feature in rolling_fea:\n                rolling_columns[f'{num}D_AVG_{feature}'] = df_cleaned[feature].shift(1).rolling(num).mean()\n                rolling_columns[f'{num}D_STD_{feature}'] = df_cleaned[feature].shift(1).rolling(num).std()\n            for feature in ['icon_partly-cloudy-day', 'icon_rain']:\n                rolling_columns[f'{num}D_AVG_{feature}'] = df_cleaned[feature].shift(1).rolling(num).mean()\n        rolling_columns_df = pd.DataFrame(rolling_columns)\n        df_cleaned = pd.concat([df_cleaned, rolling_columns_df], axis = 1)   \n\n        # df_fe = df_cleaned[['temp'] + time_features]\n        full_features = ['temp', 'datetime']  + list(derived_df.columns) + list(time_df.columns) + list(season_df.columns) + list(rolling_columns_df.columns)\n        df_fe = df_cleaned[full_features]\n    \n        # df_fe = df_fe.replace([np.inf, -np.inf], np.nan)\n        # df_fe = df_fe.select_dtypes(include=[np.number]).fillna(0)\n        # # 3. Drop NaNs \n        # print(f'num of na: {df_fe.isna().sum()}')\n        df_fe = df_fe.fillna(0)\n\n\n        return df_fe\n    \n    \n\n"},"outputs":[],"source":"def feature_eng(df):\n        df = df.copy().sort_values(by = ['datetime'])\n        #drop uneeded data\n        columns_to_drop = [\n            'name', 'snow', 'snowdepth', 'name', 'stations', 'conditions','description', 'severerisk', 'sunset', 'sunrise', 'moonphase', 'precipprob', 'uvindex'\n        ]\n        df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n\n        #create time feature\n        df_cleaned['month'] = df_cleaned['datetime'].dt.month\n        df_cleaned['dfy'] = df_cleaned['datetime'].dt.dayofyear\n        # df_cleaned['year'] = df_cleaned['datetime'].dt.year\n        # make sense the characteristics\n        df_cleaned['month_sin'] = np.sin(2 * np.pi * df_cleaned['month'] / 12)\n        df_cleaned['month_cos'] = np.cos(2 * np.pi * df_cleaned['month'] / 12)\n        df_cleaned['dfy_sin'] = np.sin(2 * np.pi * df_cleaned['dfy'] / 365)\n        df_cleaned['dfy_cos'] = np.cos(2 * np.pi * df_cleaned['dfy'] / 365)\n\n        time_df = df_cleaned[['month_sin', 'month_cos', 'dfy_sin', 'dfy_cos']]\n        \n        # ROLLING FEATURES\n        df_cleaned['winddir_sin'] = np.sin(np.deg2rad(df_cleaned['winddir']))\n        df_cleaned['winddir_cos'] = np.cos(np.deg2rad(df_cleaned['winddir']))\n        rolling_fea = ['winddir_cos', 'winddir_sin', 'dew', 'humidity', 'precip', 'precipcover', 'visibility', 'solarenergy', 'cloudcover', 'windspeed']\n        \n        # DERIVED FEATURES\n        derived = {}\n        derived['temp_range_lag1'] = df_cleaned['tempmax'].shift(1) - df_cleaned['tempmin'].shift(1)\n        derived['dew_temp_diff'] = df_cleaned['dew'].shift(1) - df_cleaned['temp'].shift(1)\n        derived['solar_per_cloud'] = df_cleaned['solarenergy'].shift(1) * (1- df_cleaned['cloudcover'].shift(1))/100\n        derived['humid_rad_ratio'] = df_cleaned['humidity'].shift(1)/ (df_cleaned['solarradiation'].shift(1)+1e-6)\n        derived['wind_humidity_interaction'] = df_cleaned['humidity'].shift(1) * (df_cleaned['windspeed'].shift(1)) / 100\n        derived['temp_humid'] = df_cleaned['temp'].shift(1) * df_cleaned['humidity'].shift(1)\n        derived['heat_index'] = df_cleaned['feelslike'].shift(1) - df_cleaned['temp'].shift(1)\n        derived['flmax_humid'] = df_cleaned['feelslikemax'].shift(1) * df_cleaned['humidity'].shift(1)/100\n        derived['flmin_cloud'] = df_cleaned['feelslikemin'].shift(1) * df_cleaned['cloudcover'].shift(1)/100\n        derived['sea_level_pressure_tendency'] = df_cleaned['sealevelpressure'].shift(1) - df_cleaned['sealevelpressure'].shift(6)\n        #df_cleaned['wind_dir'].head(10)\n        # interaction of solar energy and cloud cover\n        derived_df = pd.DataFrame(derived)\n        df_cleaned = pd.concat([df_cleaned, derived_df], axis=1)\n        \n        \n        \n        #onehot encode\n        nominal_cols = ['icon','preciptype']\n        df_encoded = pd.get_dummies(df_cleaned[nominal_cols], drop_first = True).astype(int)\n        df_no_cat = df_cleaned.drop(columns = nominal_cols, errors = 'ignore')\n        # Combine encoded features\n        df_cleaned = pd.concat([df_no_cat, df_encoded], axis = 1)\n        # print(df_cleaned.columns)\n        \n        # Season features\n        season = {}\n        for feature in ['humidity', 'dew', 'precip', 'windspeed']:\n            season[f'{feature}_seasonal'] = df_cleaned[feature].shift(1).rolling(3).max() - df_cleaned[feature].shift(1).rolling(7).max()\n            season[f'{feature}_trend'] = df_cleaned[feature].shift(1) - df_cleaned[feature].shift(2)\n            season[f'{feature}_derivative'] = season[f'{feature}_trend'].shift(1) - season [f'{feature}_trend'].shift(2)\n\n        season_df = pd.DataFrame(season)\n        df_cleaned = pd.concat([df_cleaned, season_df], axis = 1)\n        \n        rolling_columns = {}\n        for num in [7, 21, 42, 84, 126, 182]:\n            for feature in rolling_fea:\n                rolling_columns[f'{num}D_AVG_{feature}'] = df_cleaned[feature].shift(1).rolling(num).mean()\n                rolling_columns[f'{num}D_STD_{feature}'] = df_cleaned[feature].shift(1).rolling(num).std()\n            for feature in ['icon_partly-cloudy-day', 'icon_rain']:\n                rolling_columns[f'{num}D_AVG_{feature}'] = df_cleaned[feature].shift(1).rolling(num).mean()\n        rolling_columns_df = pd.DataFrame(rolling_columns)\n        df_cleaned = pd.concat([df_cleaned, rolling_columns_df], axis = 1)   \n\n        # df_fe = df_cleaned[['temp'] + time_features]\n        full_features = ['temp', 'datetime']  + list(derived_df.columns) + list(time_df.columns) + list(season_df.columns) + list(rolling_columns_df.columns)\n        df_fe = df_cleaned[full_features]\n    \n        # df_fe = df_fe.replace([np.inf, -np.inf], np.nan)\n        # df_fe = df_fe.select_dtypes(include=[np.number]).fillna(0)\n        # # 3. Drop NaNs \n        # print(f'num of na: {df_fe.isna().sum()}')\n        df_fe = df_fe.fillna(0)\n\n\n        return df_fe\n    \n    \n\n"},{"block_group":"a6f68dc4c57b4c65803b95d8559d4c26","cell_type":"code","execution_count":5,"metadata":{"execution_start":1763392940213,"execution_millis":92,"source_hash":"195797cd","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"c0245243c2bc4c5d9ada79b907d3db38","deepnote_block_group":"a6f68dc4c57b4c65803b95d8559d4c26","deepnote_cell_type":"code","deepnote_sorting_key":"10","deepnote_content_hash":"195797cd","deepnote_execution_started_at":"2025-11-17T15:22:20.213Z","deepnote_execution_finished_at":"2025-11-17T15:22:20.305Z","deepnote_source":"train_end = pd.Timestamp(\"2023-06-30\")\ngap_months = 9\ntest_start = train_end + pd.DateOffset(months=gap_months)\n\ndf_cleaned_fe = feature_eng(df)\n\n# === Chu·∫©n b·ªã d·ªØ li·ªáu d·ª± ƒëo√°n 5 ng√†y ===\ndf_multi = df_cleaned_fe.copy()\n\n# T·∫°o c√°c target d, d+1, d+2, d+3, d+4\ndf_multi['temp_d']   = df_multi['temp']          # h√¥m nay\ndf_multi['temp_d+1'] = df_multi['temp'].shift(-1)\ndf_multi['temp_d+2'] = df_multi['temp'].shift(-2)\ndf_multi['temp_d+3'] = df_multi['temp'].shift(-3)\ndf_multi['temp_d+4'] = df_multi['temp'].shift(-4)\n\n# Features\nX_multi = df_multi.drop(columns=['temp','temp_d','temp_d+1','temp_d+2','temp_d+3','temp_d+4','datetime'])\ny_multi = df_multi[['temp_d','temp_d+1','temp_d+2','temp_d+3','temp_d+4']]\ndates = df_multi['datetime']\n\n# Chia train/test theo th·ªùi gian\nX_train_multi = X_multi[dates <= train_end].fillna(0)\ny_train_multi = y_multi[dates <= train_end]\n\nX_test_multi  = X_multi[dates >= test_start].fillna(0)\ny_test_multi  = y_multi[dates >= test_start]\n\n#  Lo·∫°i b·ªè c√°c row trong y_train/y_test c√≥ NaN ƒë·ªÉ tr√°nh l·ªói XGBoost\ntrain_valid_idx = y_train_multi.dropna().index\nX_train_multi = X_train_multi.loc[train_valid_idx]\ny_train_multi = y_train_multi.loc[train_valid_idx]\n\ntest_valid_idx = y_test_multi.dropna().index\nX_test_multi = X_test_multi.loc[test_valid_idx]\ny_test_multi = y_test_multi.loc[test_valid_idx]\n# Before calling convert_all_models_from_notebook(...)\n\n# Convert X dataframes/arrays to float type, ensuring no strings remain\nX_train_multi = X_train_multi.astype(np.float32)\nX_test_multi = X_test_multi.astype(np.float32)\n\n# Also ensure Y (labels) are correct integers or floats depending on the task\ny_train_multi = y_train_multi.astype(np.int32) \ny_test_multi = y_test_multi.astype(np.int32) \n# or \n# y_train_multi = y_train_multi.astype(np.float32)\n"},"outputs":[],"source":"train_end = pd.Timestamp(\"2023-06-30\")\ngap_months = 9\ntest_start = train_end + pd.DateOffset(months=gap_months)\n\ndf_cleaned_fe = feature_eng(df)\n\n# === Chu·∫©n b·ªã d·ªØ li·ªáu d·ª± ƒëo√°n 5 ng√†y ===\ndf_multi = df_cleaned_fe.copy()\n\n# T·∫°o c√°c target d, d+1, d+2, d+3, d+4\ndf_multi['temp_d']   = df_multi['temp']          # h√¥m nay\ndf_multi['temp_d+1'] = df_multi['temp'].shift(-1)\ndf_multi['temp_d+2'] = df_multi['temp'].shift(-2)\ndf_multi['temp_d+3'] = df_multi['temp'].shift(-3)\ndf_multi['temp_d+4'] = df_multi['temp'].shift(-4)\n\n# Features\nX_multi = df_multi.drop(columns=['temp','temp_d','temp_d+1','temp_d+2','temp_d+3','temp_d+4','datetime'])\ny_multi = df_multi[['temp_d','temp_d+1','temp_d+2','temp_d+3','temp_d+4']]\ndates = df_multi['datetime']\n\n# Chia train/test theo th·ªùi gian\nX_train_multi = X_multi[dates <= train_end].fillna(0)\ny_train_multi = y_multi[dates <= train_end]\n\nX_test_multi  = X_multi[dates >= test_start].fillna(0)\ny_test_multi  = y_multi[dates >= test_start]\n\n#  Lo·∫°i b·ªè c√°c row trong y_train/y_test c√≥ NaN ƒë·ªÉ tr√°nh l·ªói XGBoost\ntrain_valid_idx = y_train_multi.dropna().index\nX_train_multi = X_train_multi.loc[train_valid_idx]\ny_train_multi = y_train_multi.loc[train_valid_idx]\n\ntest_valid_idx = y_test_multi.dropna().index\nX_test_multi = X_test_multi.loc[test_valid_idx]\ny_test_multi = y_test_multi.loc[test_valid_idx]\n# Before calling convert_all_models_from_notebook(...)\n\n# Convert X dataframes/arrays to float type, ensuring no strings remain\nX_train_multi = X_train_multi.astype(np.float32)\nX_test_multi = X_test_multi.astype(np.float32)\n\n# Also ensure Y (labels) are correct integers or floats depending on the task\ny_train_multi = y_train_multi.astype(np.int32) \ny_test_multi = y_test_multi.astype(np.int32) \n# or \n# y_train_multi = y_train_multi.astype(np.float32)\n"},{"block_group":"01ff1c8ef6984ba6bf8a659f6115f7a1","cell_type":"code","execution_count":6,"metadata":{"execution_start":1763392940365,"execution_millis":172330,"source_hash":"52e54ced","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"3d91c036320d4bfc85d6ad85b014d8c1","deepnote_block_group":"01ff1c8ef6984ba6bf8a659f6115f7a1","deepnote_cell_type":"code","deepnote_sorting_key":"11","deepnote_content_hash":"52e54ced","deepnote_execution_started_at":"2025-11-17T15:22:20.365Z","deepnote_execution_finished_at":"2025-11-17T15:25:12.695Z","deepnote_source":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n\n\nbase_models = [\n    (\"XGBoost\", XGBRegressor(\n        n_estimators=700,\n        learning_rate=0.01211,\n        max_depth=2,\n        min_child_weight=3,\n        subsample=0.53156,\n        colsample_bytree=0.52213,\n        reg_alpha=0.05247,\n        reg_lambda=0.00000128,\n        random_state=42,\n        early_stopping_rounds=100,\n    )\n    ),\n    \n    (\"CatBoost\", CatBoostRegressor(\n        iterations=1000, learning_rate=0.05, depth=6, loss_function='RMSE',\n        random_seed=42, eval_metric='RMSE', verbose=False\n    )),\n    \n    (\"AdaBoost\", AdaBoostRegressor(\n        n_estimators=50,\n        learning_rate=0.02765299922596566,\n        random_state=42\n    )\n    )\n]\n\n\nday_targets = ['temp_d','temp_d+1','temp_d+2','temp_d+3','temp_d+4']\nday_labels  = ['Day 0','Day 1','Day 2','Day 3','Day 4']\n\n\n# === V√íNG FOR TRAIN T·∫§T C·∫¢ MODEL ===\nall_results = []\n\nfor i, target in enumerate(day_targets):\n    y_train_day = y_train_multi[target]\n    y_test_day  = y_test_multi[target]\n\n    for name, model in base_models:\n        # Fit model\n        if name == \"XGBoost\":\n            model.fit(X_train_multi, y_train_day,\n                      eval_set=[(X_train_multi, y_train_day), (X_test_multi, y_test_day)],\n                      verbose=False)\n        elif name == \"CatBoost\":\n            model.fit(X_train_multi, y_train_day,\n                      eval_set=(X_test_multi, y_test_day),\n                      use_best_model=True)\n        else:\n            model.fit(X_train_multi, y_train_day)\n        \n        # Predict\n        y_pred_train = model.predict(X_train_multi)\n        y_pred_test  = model.predict(X_test_multi)\n\n       "},"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n\n\nbase_models = [\n    (\"XGBoost\", XGBRegressor(\n        n_estimators=700,\n        learning_rate=0.01211,\n        max_depth=2,\n        min_child_weight=3,\n        subsample=0.53156,\n        colsample_bytree=0.52213,\n        reg_alpha=0.05247,\n        reg_lambda=0.00000128,\n        random_state=42,\n        early_stopping_rounds=100,\n    )\n    ),\n    \n    (\"CatBoost\", CatBoostRegressor(\n        iterations=1000, learning_rate=0.05, depth=6, loss_function='RMSE',\n        random_seed=42, eval_metric='RMSE', verbose=False\n    )),\n    \n    (\"AdaBoost\", AdaBoostRegressor(\n        n_estimators=50,\n        learning_rate=0.02765299922596566,\n        random_state=42\n    )\n    )\n]\n\n\nday_targets = ['temp_d','temp_d+1','temp_d+2','temp_d+3','temp_d+4']\nday_labels  = ['Day 0','Day 1','Day 2','Day 3','Day 4']\n\n\n# === V√íNG FOR TRAIN T·∫§T C·∫¢ MODEL ===\nall_results = []\n\nfor i, target in enumerate(day_targets):\n    y_train_day = y_train_multi[target]\n    y_test_day  = y_test_multi[target]\n\n    for name, model in base_models:\n        # Fit model\n        if name == \"XGBoost\":\n            model.fit(X_train_multi, y_train_day,\n                      eval_set=[(X_train_multi, y_train_day), (X_test_multi, y_test_day)],\n                      verbose=False)\n        elif name == \"CatBoost\":\n            model.fit(X_train_multi, y_train_day,\n                      eval_set=(X_test_multi, y_test_day),\n                      use_best_model=True)\n        else:\n            model.fit(X_train_multi, y_train_day)\n        \n        # Predict\n        y_pred_train = model.predict(X_train_multi)\n        y_pred_test  = model.predict(X_test_multi)\n\n       "},{"block_group":"0abc41465f4b47bcbae3d4320e50918a","cell_type":"code","execution_count":7,"metadata":{"execution_start":1763393112754,"execution_millis":18541,"source_hash":"85954658","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"c84df2d6ffe64845935650b6eca1036f","deepnote_block_group":"0abc41465f4b47bcbae3d4320e50918a","deepnote_cell_type":"code","deepnote_sorting_key":"12","deepnote_content_hash":"85954658","deepnote_execution_started_at":"2025-11-17T15:25:12.754Z","deepnote_execution_finished_at":"2025-11-17T15:25:31.295Z","deepnote_source":"!pip install onnx --quiet\n!pip install onnxruntime --quiet"},"outputs":[{"name":"stdout","text":"\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"source":"!pip install onnx --quiet\n!pip install onnxruntime --quiet"},{"block_group":"19dd7af523094566b43f092cb79c71de","cell_type":"code","execution_count":8,"metadata":{"execution_start":1763393131343,"execution_millis":3738,"source_hash":"98d8b7d7","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"5fa84f1b82194da091caa8371fefab1a","deepnote_block_group":"19dd7af523094566b43f092cb79c71de","deepnote_cell_type":"code","deepnote_sorting_key":"13","deepnote_content_hash":"98d8b7d7","deepnote_execution_started_at":"2025-11-17T15:25:31.343Z","deepnote_execution_finished_at":"2025-11-17T15:25:35.081Z","deepnote_source":"pip install onnx onnxruntime skl2onnx onnxmltools --quiet"},"outputs":[{"name":"stdout","text":"\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"source":"pip install onnx onnxruntime skl2onnx onnxmltools --quiet"},{"block_group":"fc24457c50194b22b981dfef2ec5ab37","cell_type":"code","execution_count":17,"metadata":{"execution_start":1763393280869,"execution_millis":2,"source_hash":"ad4667e7","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"d52d873d7a084a6fb2fdd3115bbddf53","deepnote_block_group":"fc24457c50194b22b981dfef2ec5ab37","deepnote_cell_type":"code","deepnote_sorting_key":"14","deepnote_content_hash":"ad4667e7","deepnote_execution_started_at":"2025-11-17T15:28:00.869Z","deepnote_execution_finished_at":"2025-11-17T15:28:00.871Z","deepnote_source":"\"\"\"\nConvert trained ML models to ONNX format for efficient deployment\nSupports: XGBoost, CatBoost, LightGBM, RandomForest, AdaBoost, DecisionTree\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport onnx\nimport onnxruntime as ort\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom onnxmltools.convert import convert_xgboost, convert_lightgbm\nfrom onnxmltools.convert.common.data_types import FloatTensorType as OnnxFloatTensorType\nimport os\nclass ModelToONNXConverter:\n    \"\"\"\n    Convert various ML models to ONNX format\n    \"\"\"\n    \n    def __init__(self, n_features, model_save_dir='onnx_models'):\n        \"\"\"\n        Initialize converter\n        \n        Args:\n            n_features: Number of input features\n            model_save_dir: Directory to save ONNX models\n        \"\"\"\n        self.n_features = n_features\n        self.model_save_dir = model_save_dir\n        \n        # Create directory if it doesn't exist\n        os.makedirs(model_save_dir, exist_ok=True)\n        \n    def convert_xgboost(self, model, model_name, target_opset=12):\n        \"\"\"Convert XGBoost model to ONNX\"\"\"\n        try:\n            initial_type = [('float_input', OnnxFloatTensorType([None, self.n_features]))]\n            onnx_model = convert_xgboost(\n                model,\n                initial_types=initial_type,\n                target_opset=target_opset\n            )\n            save_path = os.path.join(self.model_save_dir, f\"{model_name}.onnx\")\n            onnx.save_model(onnx_model, save_path)\n            print(f\"‚úÖ XGBoost model saved: {save_path}\")\n            return save_path\n        except Exception as e:\n            print(f\"‚ùå Error converting XGBoost: {e}\")\n            return None\n        \n    \n    def convert_lightgbm(self, model, model_name, target_opset=12):\n        \"\"\"Convert LightGBM model to ONNX\"\"\"\n        try:\n            initial_type = [('float_input', OnnxFloatTensorType([None, self.n_features]))]\n            onnx_model = convert_lightgbm(\n                model,\n                initial_types=initial_type,\n                target_opset=target_opset\n            )\n            \n            save_path = os.path.join(self.model_save_dir, f\"{model_name}.onnx\")\n            onnx.save_model(onnx_model, save_path)\n            print(f\"‚úÖ LightGBM model saved: {save_path}\")\n            return save_path\n        except Exception as e:\n            print(f\"‚ùå Error converting LightGBM: {e}\")\n            return None\n    \n    def convert_catboost(self, model, model_name):\n        \"\"\"Convert CatBoost model to ONNX\"\"\"\n        try:\n            # CatBoost has built-in ONNX export\n            save_path = os.path.join(self.model_save_dir, f\"{model_name}.onnx\")\n            model.save_model(\n                save_path,\n                format=\"onnx\",\n                export_parameters={\n                    'onnx_domain': 'ai.catboost',\n                    'onnx_model_version': 1,\n                    'onnx_doc_string': 'CatBoost model for temperature prediction',\n                    'onnx_graph_name': 'CatBoostModel'\n                }\n            )\n            print(f\"‚úÖ CatBoost model saved: {save_path}\")\n            return save_path\n        except Exception as e:\n            print(f\"‚ùå Error converting CatBoost: {e}\")\n            return None\n    \n    def convert_sklearn(self, model, model_name, target_opset=12):\n        \"\"\"Convert sklearn-based models (RandomForest, AdaBoost, DecisionTree) to ONNX\"\"\"\n        try:\n            initial_type = [('float_input', FloatTensorType([None, self.n_features]))]\n            onnx_model = convert_sklearn(\n                model,\n                initial_types=initial_type,\n                target_opset=target_opset\n            )\n            \n            save_path = os.path.join(self.model_save_dir, f\"{model_name}.onnx\")\n            onnx.save_model(onnx_model, save_path)\n            print(f\"‚úÖ Sklearn model saved: {save_path}\")\n            return save_path\n        except Exception as e:\n            print(f\"‚ùå Error converting Sklearn model: {e}\")\n            return None\n    \n    def convert_model(self, model, model_name, model_type):\n        \"\"\"\n        Universal converter that routes to appropriate conversion method\n        \n        Args:\n            model: Trained model object\n            model_name: Name for saving the model\n            model_type: Type of model ('xgboost', 'lightgbm', 'catboost', 'sklearn')\n        \n        Returns:\n            Path to saved ONNX model or None if failed\n        \"\"\"\n        model_type = model_type.lower()\n        \n        if model_type == 'xgboost':\n            return self.convert_xgboost(model, model_name)\n        elif model_type == 'lightgbm':\n            return self.convert_lightgbm(model, model_name)\n        elif model_type == 'catboost':\n            return self.convert_catboost(model, model_name)\n        elif model_type in ['sklearn', 'randomforest', 'adaboost', 'decisiontree']:\n            return self.convert_sklearn(model, model_name)\n        else:\n            print(f\"‚ùå Unknown model type: {model_type}\")\n            return None\n    \n    def verify_onnx_model(self, onnx_path, X_test_sample):\n        \"\"\"\n        Verify ONNX model by running inference\n        \n        Args:\n            onnx_path: Path to ONNX model\n            X_test_sample: Sample data for testing (numpy array)\n        \n        Returns:\n            Prediction results or None if failed\n        \"\"\"\n        try:\n            # Load ONNX model\n            ort_session = ort.InferenceSession(onnx_path)\n            \n            # Get input name\n            input_name = ort_session.get_inputs()[0].name\n            \n            # Run inference\n            ort_inputs = {input_name: X_test_sample.astype(np.float32)}\n            ort_outputs = ort_session.run(None, ort_inputs)\n            \n            print(f\"‚úÖ ONNX model verified: {onnx_path}\")\n            print(f\"   Output shape: {ort_outputs[0].shape}\")\n            return ort_outputs[0]\n        except Exception as e:\n            print(f\"‚ùå Error verifying ONNX model: {e}\")\n            return None\n\n\n# ============================================================================\n# EXAMPLE USAGE WITH YOUR NOTEBOOK'S MODELS\n# ============================================================================\n\ndef convert_all_models_from_notebook(X_train_multi, y_train_multi, X_test_multi, y_test_multi):\n    \"\"\"\n    Train and convert all models from your notebook to ONNX\n    \"\"\"\n    \n    # Initialize converter\n    n_features = X_train_multi.shape[1]\n    converter = ModelToONNXConverter(n_features=n_features, model_save_dir='onnx_models')\n    \n    # Define models from your notebook\n    base_models = [\n        (\"XGBoost\", XGBRegressor(\n            n_estimators=600, learning_rate=0.05, max_depth=3, min_child_weight=5,\n            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n            random_state=42\n        ), \"xgboost\"),\n        \n        (\"CatBoost\", CatBoostRegressor(\n            iterations=1000, learning_rate=0.05, depth=6, loss_function='RMSE',\n            random_seed=42, eval_metric='RMSE', verbose=False\n        ), \"catboost\"),\n        \n        (\"AdaBoost\", AdaBoostRegressor(\n            n_estimators=400, learning_rate=0.05, random_state=42\n        ), \"sklearn\"),\n        \n        (\"RandomForest\", RandomForestRegressor(\n            n_estimators=200, max_depth=10, min_samples_split=22,\n            min_samples_leaf=9, random_state=42, n_jobs=-1\n        ), \"sklearn\"),\n        \n        (\"DecisionTree\", DecisionTreeRegressor(\n            max_depth=6, min_samples_split=20, min_samples_leaf=10,\n            random_state=42\n        ), \"sklearn\")\n    ]\n    \n    # Target columns for 5-day prediction\n    day_targets = ['temp_d', 'temp_d+1', 'temp_d+2', 'temp_d+3', 'temp_d+4']\n    day_labels = ['Day0', 'Day1', 'Day2', 'Day3', 'Day4']\n    \n    converted_models = []\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üöÄ STARTING MODEL TRAINING AND ONNX CONVERSION\")\n    print(\"=\"*70)\n    \n    # Loop through each day prediction\n    for i, (target, day_label) in enumerate(zip(day_targets, day_labels)):\n        print(f\"\\nüìÖ Processing {day_label} ({target})\")\n        print(\"-\" * 70)\n        \n        y_train_day = y_train_multi[target]\n        y_test_day = y_test_multi[target]\n        \n        # Loop through each model\n        for model_name, model, model_type in base_models:\n            print(f\"\\nüîß Training {model_name} for {day_label}...\")\n            \n            # Train model\n            if model_type == \"xgboost\":\n                model.fit(X_train_multi, y_train_day, verbose=False)\n            elif model_type == \"catboost\":\n                model.fit(X_train_multi, y_train_day, verbose=False)\n            else:\n                model.fit(X_train_multi, y_train_day)\n            \n            # Convert to ONNX\n            onnx_name = f\"{model_name}_{day_label}\"\n            onnx_path = converter.convert_model(model, onnx_name, model_type)\n            \n            if onnx_path:\n                # Verify conversion\n                X_sample = X_test_multi[:5].values if hasattr(X_test_multi, 'values') else X_test_multi[:5]\n                converter.verify_onnx_model(onnx_path, X_sample)\n                \n                converted_models.append({\n                    'day': day_label,\n                    'model': model_name,\n                    'onnx_path': onnx_path,\n                    'status': 'success'\n                })\n            else:\n                converted_models.append({\n                    'day': day_label,\n                    'model': model_name,\n                    'onnx_path': None,\n                    'status': 'failed'\n                })\n    \n    # Summary\n    print(\"\\n\" + \"=\"*70)\n    print(\"üìä CONVERSION SUMMARY\")\n    print(\"=\"*70)\n    \n    df_summary = pd.DataFrame(converted_models)\n    print(df_summary)\n    \n    success_count = df_summary[df_summary['status'] == 'success'].shape[0]\n    total_count = df_summary.shape[0]\n    print(f\"\\n‚úÖ Successfully converted: {success_count}/{total_count} models\")\n    \n    return converted_models, df_summary\n\n\n# ============================================================================\n# STANDALONE CONVERSION FUNCTION (if you already have trained models)\n# ============================================================================\n\ndef convert_single_model_to_onnx(model, model_name, model_type, n_features, save_dir='onnx_models'):\n    \"\"\"\n    Convert a single trained model to ONNX\n    \n    Args:\n        model: Trained model object\n        model_name: Name for the ONNX file\n        model_type: 'xgboost', 'lightgbm', 'catboost', or 'sklearn'\n        n_features: Number of input features\n        save_dir: Directory to save the model\n    \n    Returns:\n        Path to saved ONNX model\n    \"\"\"\n    converter = ModelToONNXConverter(n_features=n_features, model_save_dir=save_dir)\n    return converter.convert_model(model, model_name, model_type)\n\n\n# ============================================================================\n# USAGE EXAMPLE\n# ============================================================================\n\nif __name__ == \"__main__\":\n    \"\"\"\n    To use this script with your notebook data:\n    \n    1. Make sure you have the required packages installed:\n       pip install onnx onnxruntime skl2onnx onnxmltools\n    \n    2. Load your data (from your notebook):\n       - X_train_multi, y_train_multi, X_test_multi, y_test_multi\n    \n    3. Run the conversion:\n       converted_models, summary = convert_all_models_from_notebook(\n           X_train_multi, y_train_multi, X_test_multi, y_test_multi\n       )\n    \n    4. Your ONNX models will be saved in the 'onnx_models' directory\n    \"\"\"\n    \n    print(\"üìñ ONNX Model Converter Ready!\")\n    print(\"\\nTo convert your models, call:\")\n    print(\"convert_all_models_from_notebook(X_train_multi, y_train_multi, X_test_multi, y_test_multi)\")"},"outputs":[{"name":"stdout","text":"üìñ ONNX Model Converter Ready!\n\nTo convert your models, call:\nconvert_all_models_from_notebook(X_train_multi, y_train_multi, X_test_multi, y_test_multi)\n","output_type":"stream"}],"source":"\"\"\"\nConvert trained ML models to ONNX format for efficient deployment\nSupports: XGBoost, CatBoost, LightGBM, RandomForest, AdaBoost, DecisionTree\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport onnx\nimport onnxruntime as ort\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom onnxmltools.convert import convert_xgboost, convert_lightgbm\nfrom onnxmltools.convert.common.data_types import FloatTensorType as OnnxFloatTensorType\nimport os\nclass ModelToONNXConverter:\n    \"\"\"\n    Convert various ML models to ONNX format\n    \"\"\"\n    \n    def __init__(self, n_features, model_save_dir='onnx_models'):\n        \"\"\"\n        Initialize converter\n        \n        Args:\n            n_features: Number of input features\n            model_save_dir: Directory to save ONNX models\n        \"\"\"\n        self.n_features = n_features\n        self.model_save_dir = model_save_dir\n        \n        # Create directory if it doesn't exist\n        os.makedirs(model_save_dir, exist_ok=True)\n        \n    def convert_xgboost(self, model, model_name, target_opset=12):\n        \"\"\"Convert XGBoost model to ONNX\"\"\"\n        try:\n            initial_type = [('float_input', OnnxFloatTensorType([None, self.n_features]))]\n            onnx_model = convert_xgboost(\n                model,\n                initial_types=initial_type,\n                target_opset=target_opset\n            )\n            save_path = os.path.join(self.model_save_dir, f\"{model_name}.onnx\")\n            onnx.save_model(onnx_model, save_path)\n            print(f\"‚úÖ XGBoost model saved: {save_path}\")\n            return save_path\n        except Exception as e:\n            print(f\"‚ùå Error converting XGBoost: {e}\")\n            return None\n        \n    \n    def convert_lightgbm(self, model, model_name, target_opset=12):\n        \"\"\"Convert LightGBM model to ONNX\"\"\"\n        try:\n            initial_type = [('float_input', OnnxFloatTensorType([None, self.n_features]))]\n            onnx_model = convert_lightgbm(\n                model,\n                initial_types=initial_type,\n                target_opset=target_opset\n            )\n            \n            save_path = os.path.join(self.model_save_dir, f\"{model_name}.onnx\")\n            onnx.save_model(onnx_model, save_path)\n            print(f\"‚úÖ LightGBM model saved: {save_path}\")\n            return save_path\n        except Exception as e:\n            print(f\"‚ùå Error converting LightGBM: {e}\")\n            return None\n    \n    def convert_catboost(self, model, model_name):\n        \"\"\"Convert CatBoost model to ONNX\"\"\"\n        try:\n            # CatBoost has built-in ONNX export\n            save_path = os.path.join(self.model_save_dir, f\"{model_name}.onnx\")\n            model.save_model(\n                save_path,\n                format=\"onnx\",\n                export_parameters={\n                    'onnx_domain': 'ai.catboost',\n                    'onnx_model_version': 1,\n                    'onnx_doc_string': 'CatBoost model for temperature prediction',\n                    'onnx_graph_name': 'CatBoostModel'\n                }\n            )\n            print(f\"‚úÖ CatBoost model saved: {save_path}\")\n            return save_path\n        except Exception as e:\n            print(f\"‚ùå Error converting CatBoost: {e}\")\n            return None\n    \n    def convert_sklearn(self, model, model_name, target_opset=12):\n        \"\"\"Convert sklearn-based models (RandomForest, AdaBoost, DecisionTree) to ONNX\"\"\"\n        try:\n            initial_type = [('float_input', FloatTensorType([None, self.n_features]))]\n            onnx_model = convert_sklearn(\n                model,\n                initial_types=initial_type,\n                target_opset=target_opset\n            )\n            \n            save_path = os.path.join(self.model_save_dir, f\"{model_name}.onnx\")\n            onnx.save_model(onnx_model, save_path)\n            print(f\"‚úÖ Sklearn model saved: {save_path}\")\n            return save_path\n        except Exception as e:\n            print(f\"‚ùå Error converting Sklearn model: {e}\")\n            return None\n    \n    def convert_model(self, model, model_name, model_type):\n        \"\"\"\n        Universal converter that routes to appropriate conversion method\n        \n        Args:\n            model: Trained model object\n            model_name: Name for saving the model\n            model_type: Type of model ('xgboost', 'lightgbm', 'catboost', 'sklearn')\n        \n        Returns:\n            Path to saved ONNX model or None if failed\n        \"\"\"\n        model_type = model_type.lower()\n        \n        if model_type == 'xgboost':\n            return self.convert_xgboost(model, model_name)\n        elif model_type == 'lightgbm':\n            return self.convert_lightgbm(model, model_name)\n        elif model_type == 'catboost':\n            return self.convert_catboost(model, model_name)\n        elif model_type in ['sklearn', 'randomforest', 'adaboost', 'decisiontree']:\n            return self.convert_sklearn(model, model_name)\n        else:\n            print(f\"‚ùå Unknown model type: {model_type}\")\n            return None\n    \n    def verify_onnx_model(self, onnx_path, X_test_sample):\n        \"\"\"\n        Verify ONNX model by running inference\n        \n        Args:\n            onnx_path: Path to ONNX model\n            X_test_sample: Sample data for testing (numpy array)\n        \n        Returns:\n            Prediction results or None if failed\n        \"\"\"\n        try:\n            # Load ONNX model\n            ort_session = ort.InferenceSession(onnx_path)\n            \n            # Get input name\n            input_name = ort_session.get_inputs()[0].name\n            \n            # Run inference\n            ort_inputs = {input_name: X_test_sample.astype(np.float32)}\n            ort_outputs = ort_session.run(None, ort_inputs)\n            \n            print(f\"‚úÖ ONNX model verified: {onnx_path}\")\n            print(f\"   Output shape: {ort_outputs[0].shape}\")\n            return ort_outputs[0]\n        except Exception as e:\n            print(f\"‚ùå Error verifying ONNX model: {e}\")\n            return None\n\n\n# ============================================================================\n# EXAMPLE USAGE WITH YOUR NOTEBOOK'S MODELS\n# ============================================================================\n\ndef convert_all_models_from_notebook(X_train_multi, y_train_multi, X_test_multi, y_test_multi):\n    \"\"\"\n    Train and convert all models from your notebook to ONNX\n    \"\"\"\n    \n    # Initialize converter\n    n_features = X_train_multi.shape[1]\n    converter = ModelToONNXConverter(n_features=n_features, model_save_dir='onnx_models')\n    \n    # Define models from your notebook\n    base_models = [\n        (\"XGBoost\", XGBRegressor(\n            n_estimators=600, learning_rate=0.05, max_depth=3, min_child_weight=5,\n            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n            random_state=42\n        ), \"xgboost\"),\n        \n        (\"CatBoost\", CatBoostRegressor(\n            iterations=1000, learning_rate=0.05, depth=6, loss_function='RMSE',\n            random_seed=42, eval_metric='RMSE', verbose=False\n        ), \"catboost\"),\n        \n        (\"AdaBoost\", AdaBoostRegressor(\n            n_estimators=400, learning_rate=0.05, random_state=42\n        ), \"sklearn\"),\n        \n        (\"RandomForest\", RandomForestRegressor(\n            n_estimators=200, max_depth=10, min_samples_split=22,\n            min_samples_leaf=9, random_state=42, n_jobs=-1\n        ), \"sklearn\"),\n        \n        (\"DecisionTree\", DecisionTreeRegressor(\n            max_depth=6, min_samples_split=20, min_samples_leaf=10,\n            random_state=42\n        ), \"sklearn\")\n    ]\n    \n    # Target columns for 5-day prediction\n    day_targets = ['temp_d', 'temp_d+1', 'temp_d+2', 'temp_d+3', 'temp_d+4']\n    day_labels = ['Day0', 'Day1', 'Day2', 'Day3', 'Day4']\n    \n    converted_models = []\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üöÄ STARTING MODEL TRAINING AND ONNX CONVERSION\")\n    print(\"=\"*70)\n    \n    # Loop through each day prediction\n    for i, (target, day_label) in enumerate(zip(day_targets, day_labels)):\n        print(f\"\\nüìÖ Processing {day_label} ({target})\")\n        print(\"-\" * 70)\n        \n        y_train_day = y_train_multi[target]\n        y_test_day = y_test_multi[target]\n        \n        # Loop through each model\n        for model_name, model, model_type in base_models:\n            print(f\"\\nüîß Training {model_name} for {day_label}...\")\n            \n            # Train model\n            if model_type == \"xgboost\":\n                model.fit(X_train_multi, y_train_day, verbose=False)\n            elif model_type == \"catboost\":\n                model.fit(X_train_multi, y_train_day, verbose=False)\n            else:\n                model.fit(X_train_multi, y_train_day)\n            \n            # Convert to ONNX\n            onnx_name = f\"{model_name}_{day_label}\"\n            onnx_path = converter.convert_model(model, onnx_name, model_type)\n            \n            if onnx_path:\n                # Verify conversion\n                X_sample = X_test_multi[:5].values if hasattr(X_test_multi, 'values') else X_test_multi[:5]\n                converter.verify_onnx_model(onnx_path, X_sample)\n                \n                converted_models.append({\n                    'day': day_label,\n                    'model': model_name,\n                    'onnx_path': onnx_path,\n                    'status': 'success'\n                })\n            else:\n                converted_models.append({\n                    'day': day_label,\n                    'model': model_name,\n                    'onnx_path': None,\n                    'status': 'failed'\n                })\n    \n    # Summary\n    print(\"\\n\" + \"=\"*70)\n    print(\"üìä CONVERSION SUMMARY\")\n    print(\"=\"*70)\n    \n    df_summary = pd.DataFrame(converted_models)\n    print(df_summary)\n    \n    success_count = df_summary[df_summary['status'] == 'success'].shape[0]\n    total_count = df_summary.shape[0]\n    print(f\"\\n‚úÖ Successfully converted: {success_count}/{total_count} models\")\n    \n    return converted_models, df_summary\n\n\n# ============================================================================\n# STANDALONE CONVERSION FUNCTION (if you already have trained models)\n# ============================================================================\n\ndef convert_single_model_to_onnx(model, model_name, model_type, n_features, save_dir='onnx_models'):\n    \"\"\"\n    Convert a single trained model to ONNX\n    \n    Args:\n        model: Trained model object\n        model_name: Name for the ONNX file\n        model_type: 'xgboost', 'lightgbm', 'catboost', or 'sklearn'\n        n_features: Number of input features\n        save_dir: Directory to save the model\n    \n    Returns:\n        Path to saved ONNX model\n    \"\"\"\n    converter = ModelToONNXConverter(n_features=n_features, model_save_dir=save_dir)\n    return converter.convert_model(model, model_name, model_type)\n\n\n# ============================================================================\n# USAGE EXAMPLE\n# ============================================================================\n\nif __name__ == \"__main__\":\n    \"\"\"\n    To use this script with your notebook data:\n    \n    1. Make sure you have the required packages installed:\n       pip install onnx onnxruntime skl2onnx onnxmltools\n    \n    2. Load your data (from your notebook):\n       - X_train_multi, y_train_multi, X_test_multi, y_test_multi\n    \n    3. Run the conversion:\n       converted_models, summary = convert_all_models_from_notebook(\n           X_train_multi, y_train_multi, X_test_multi, y_test_multi\n       )\n    \n    4. Your ONNX models will be saved in the 'onnx_models' directory\n    \"\"\"\n    \n    print(\"üìñ ONNX Model Converter Ready!\")\n    print(\"\\nTo convert your models, call:\")\n    print(\"convert_all_models_from_notebook(X_train_multi, y_train_multi, X_test_multi, y_test_multi)\")"},{"block_group":"b944688ed83745b1a6c2f89fb915ee32","cell_type":"code","execution_count":20,"metadata":{"execution_start":1763393288215,"execution_millis":494478,"source_hash":"9bb4c5fc","execution_context_id":"c7ff9ba6-2306-479e-93d7-63ee766ef0cd","cell_id":"0025527398dc4b74ba69ff4c7945c418","deepnote_block_group":"b944688ed83745b1a6c2f89fb915ee32","deepnote_cell_type":"code","deepnote_sorting_key":"15","deepnote_content_hash":"9bb4c5fc","deepnote_execution_started_at":"2025-11-17T15:28:08.215Z","deepnote_execution_finished_at":"2025-11-17T15:36:22.693Z","deepnote_source":"convert_all_models_from_notebook(X_train_multi, y_train_multi, X_test_multi, y_test_multi)"},"outputs":[{"name":"stdout","text":"\n======================================================================\nüöÄ STARTING MODEL TRAINING AND ONNX CONVERSION\n======================================================================\n\nüìÖ Processing Day0 (temp_d)\n----------------------------------------------------------------------\n\nüîß Training XGBoost for Day0...\n‚ùå Error converting XGBoost: could not convert string to float: '[2.7926846E1]'\n\nüîß Training CatBoost for Day0...\n‚úÖ CatBoost model saved: onnx_models/CatBoost_Day0.onnx\n‚úÖ ONNX model verified: onnx_models/CatBoost_Day0.onnx\n   Output shape: (5, 1)\n\nüîß Training AdaBoost for Day0...\n\u001b[0;93m2025-11-17 15:28:37.159975590 [W:onnxruntime:, execution_frame.cc:874 VerifyOutputSizes] Expected shape from model of {-1} does not match actual shape of {5,1} for output predictions\u001b[m\n‚úÖ Sklearn model saved: onnx_models/AdaBoost_Day0.onnx\n‚úÖ ONNX model verified: onnx_models/AdaBoost_Day0.onnx\n   Output shape: (5, 1)\n\nüîß Training RandomForest for Day0...\n‚úÖ Sklearn model saved: onnx_models/RandomForest_Day0.onnx\n‚úÖ ONNX model verified: onnx_models/RandomForest_Day0.onnx\n   Output shape: (5, 1)\n\nüîß Training DecisionTree for Day0...\n‚úÖ Sklearn model saved: onnx_models/DecisionTree_Day0.onnx\n‚úÖ ONNX model verified: onnx_models/DecisionTree_Day0.onnx\n   Output shape: (5, 1)\n\nüìÖ Processing Day1 (temp_d+1)\n----------------------------------------------------------------------\n\nüîß Training XGBoost for Day1...\n‚ùå Error converting XGBoost: could not convert string to float: '[2.7927168E1]'\n\nüîß Training CatBoost for Day1...\n‚úÖ CatBoost model saved: onnx_models/CatBoost_Day1.onnx\n‚úÖ ONNX model verified: onnx_models/CatBoost_Day1.onnx\n   Output shape: (5, 1)\n\nüîß Training AdaBoost for Day1...\n\u001b[0;93m2025-11-17 15:30:12.951415915 [W:onnxruntime:, execution_frame.cc:874 VerifyOutputSizes] Expected shape from model of {-1} does not match actual shape of {5,1} for output predictions\u001b[m\n‚úÖ Sklearn model saved: onnx_models/AdaBoost_Day1.onnx\n‚úÖ ONNX model verified: onnx_models/AdaBoost_Day1.onnx\n   Output shape: (5, 1)\n\nüîß Training RandomForest for Day1...\n‚úÖ Sklearn model saved: onnx_models/RandomForest_Day1.onnx\n‚úÖ ONNX model verified: onnx_models/RandomForest_Day1.onnx\n   Output shape: (5, 1)\n\nüîß Training DecisionTree for Day1...\n‚úÖ Sklearn model saved: onnx_models/DecisionTree_Day1.onnx\n‚úÖ ONNX model verified: onnx_models/DecisionTree_Day1.onnx\n   Output shape: (5, 1)\n\nüìÖ Processing Day2 (temp_d+2)\n----------------------------------------------------------------------\n\nüîß Training XGBoost for Day2...\n‚ùå Error converting XGBoost: could not convert string to float: '[2.792749E1]'\n\nüîß Training CatBoost for Day2...\n‚úÖ CatBoost model saved: onnx_models/CatBoost_Day2.onnx\n‚úÖ ONNX model verified: onnx_models/CatBoost_Day2.onnx\n   Output shape: (5, 1)\n\nüîß Training AdaBoost for Day2...\n\u001b[0;93m2025-11-17 15:31:51.099470028 [W:onnxruntime:, execution_frame.cc:874 VerifyOutputSizes] Expected shape from model of {-1} does not match actual shape of {5,1} for output predictions\u001b[m\n‚úÖ Sklearn model saved: onnx_models/AdaBoost_Day2.onnx\n‚úÖ ONNX model verified: onnx_models/AdaBoost_Day2.onnx\n   Output shape: (5, 1)\n\nüîß Training RandomForest for Day2...\n‚úÖ Sklearn model saved: onnx_models/RandomForest_Day2.onnx\n‚úÖ ONNX model verified: onnx_models/RandomForest_Day2.onnx\n   Output shape: (5, 1)\n\nüîß Training DecisionTree for Day2...\n‚úÖ Sklearn model saved: onnx_models/DecisionTree_Day2.onnx\n‚úÖ ONNX model verified: onnx_models/DecisionTree_Day2.onnx\n   Output shape: (5, 1)\n\nüìÖ Processing Day3 (temp_d+3)\n----------------------------------------------------------------------\n\nüîß Training XGBoost for Day3...\n‚ùå Error converting XGBoost: could not convert string to float: '[2.7928135E1]'\n\nüîß Training CatBoost for Day3...\n‚úÖ CatBoost model saved: onnx_models/CatBoost_Day3.onnx\n‚úÖ ONNX model verified: onnx_models/CatBoost_Day3.onnx\n   Output shape: (5, 1)\n\nüîß Training AdaBoost for Day3...\n\u001b[0;93m2025-11-17 15:33:27.226784202 [W:onnxruntime:, execution_frame.cc:874 VerifyOutputSizes] Expected shape from model of {-1} does not match actual shape of {5,1} for output predictions\u001b[m\n‚úÖ Sklearn model saved: onnx_models/AdaBoost_Day3.onnx\n‚úÖ ONNX model verified: onnx_models/AdaBoost_Day3.onnx\n   Output shape: (5, 1)\n\nüîß Training RandomForest for Day3...\n‚úÖ Sklearn model saved: onnx_models/RandomForest_Day3.onnx\n‚úÖ ONNX model verified: onnx_models/RandomForest_Day3.onnx\n   Output shape: (5, 1)\n\nüîß Training DecisionTree for Day3...\n‚úÖ Sklearn model saved: onnx_models/DecisionTree_Day3.onnx\n‚úÖ ONNX model verified: onnx_models/DecisionTree_Day3.onnx\n   Output shape: (5, 1)\n\nüìÖ Processing Day4 (temp_d+4)\n----------------------------------------------------------------------\n\nüîß Training XGBoost for Day4...\n‚ùå Error converting XGBoost: could not convert string to float: '[2.7928778E1]'\n\nüîß Training CatBoost for Day4...\n‚úÖ CatBoost model saved: onnx_models/CatBoost_Day4.onnx\n\u001b[0;93m2025-11-17 15:35:08.278904965 [W:onnxruntime:, execution_frame.cc:874 VerifyOutputSizes] Expected shape from model of {-1} does not match actual shape of {5,1} for output predictions\u001b[m\n‚úÖ ONNX model verified: onnx_models/CatBoost_Day4.onnx\n   Output shape: (5, 1)\n\nüîß Training AdaBoost for Day4...\n‚úÖ Sklearn model saved: onnx_models/AdaBoost_Day4.onnx\n‚úÖ ONNX model verified: onnx_models/AdaBoost_Day4.onnx\n   Output shape: (5, 1)\n\nüîß Training RandomForest for Day4...\n‚úÖ Sklearn model saved: onnx_models/RandomForest_Day4.onnx\n‚úÖ ONNX model verified: onnx_models/RandomForest_Day4.onnx\n   Output shape: (5, 1)\n\nüîß Training DecisionTree for Day4...\n‚úÖ Sklearn model saved: onnx_models/DecisionTree_Day4.onnx\n‚úÖ ONNX model verified: onnx_models/DecisionTree_Day4.onnx\n   Output shape: (5, 1)\n\n======================================================================\nüìä CONVERSION SUMMARY\n======================================================================\n     day         model                           onnx_path   status\n0   Day0       XGBoost                                None   failed\n1   Day0      CatBoost      onnx_models/CatBoost_Day0.onnx  success\n2   Day0      AdaBoost      onnx_models/AdaBoost_Day0.onnx  success\n3   Day0  RandomForest  onnx_models/RandomForest_Day0.onnx  success\n4   Day0  DecisionTree  onnx_models/DecisionTree_Day0.onnx  success\n5   Day1       XGBoost                                None   failed\n6   Day1      CatBoost      onnx_models/CatBoost_Day1.onnx  success\n7   Day1      AdaBoost      onnx_models/AdaBoost_Day1.onnx  success\n8   Day1  RandomForest  onnx_models/RandomForest_Day1.onnx  success\n9   Day1  DecisionTree  onnx_models/DecisionTree_Day1.onnx  success\n10  Day2       XGBoost                                None   failed\n11  Day2      CatBoost      onnx_models/CatBoost_Day2.onnx  success\n12  Day2      AdaBoost      onnx_models/AdaBoost_Day2.onnx  success\n13  Day2  RandomForest  onnx_models/RandomForest_Day2.onnx  success\n14  Day2  DecisionTree  onnx_models/DecisionTree_Day2.onnx  success\n15  Day3       XGBoost                                None   failed\n16  Day3      CatBoost      onnx_models/CatBoost_Day3.onnx  success\n17  Day3      AdaBoost      onnx_models/AdaBoost_Day3.onnx  success\n18  Day3  RandomForest  onnx_models/RandomForest_Day3.onnx  success\n19  Day3  DecisionTree  onnx_models/DecisionTree_Day3.onnx  success\n20  Day4       XGBoost                                None   failed\n21  Day4      CatBoost      onnx_models/CatBoost_Day4.onnx  success\n22  Day4      AdaBoost      onnx_models/AdaBoost_Day4.onnx  success\n23  Day4  RandomForest  onnx_models/RandomForest_Day4.onnx  success\n24  Day4  DecisionTree  onnx_models/DecisionTree_Day4.onnx  success\n\n‚úÖ Successfully converted: 20/25 models\n","output_type":"stream"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"([{'day': 'Day0', 'model': 'XGBoost', 'onnx_path': None, 'status': 'failed'},\n  {'day': 'Day0',\n   'model': 'CatBoost',\n   'onnx_path': 'onnx_models/CatBoost_Day0.onnx',\n   'status': 'success'},\n  {'day': 'Day0',\n   'model': 'AdaBoost',\n   'onnx_path': 'onnx_models/AdaBoost_Day0.onnx',\n   'status': 'success'},\n  {'day': 'Day0',\n   'model': 'RandomForest',\n   'onnx_path': 'onnx_models/RandomForest_Day0.onnx',\n   'status': 'success'},\n  {'day': 'Day0',\n   'model': 'DecisionTree',\n   'onnx_path': 'onnx_models/DecisionTree_Day0.onnx',\n   'status': 'success'},\n  {'day': 'Day1', 'model': 'XGBoost', 'onnx_path': None, 'status': 'failed'},\n  {'day': 'Day1',\n   'model': 'CatBoost',\n   'onnx_path': 'onnx_models/CatBoost_Day1.onnx',\n   'status': 'success'},\n  {'day': 'Day1',\n   'model': 'AdaBoost',\n   'onnx_path': 'onnx_models/AdaBoost_Day1.onnx',\n   'status': 'success'},\n  {'day': 'Day1',\n   'model': 'RandomForest',\n   'onnx_path': 'onnx_models/RandomForest_Day1.onnx',\n   'status': 'success'},\n  {'day': 'Day1',\n   'model': 'DecisionTree',\n   'onnx_path': 'onnx_models/DecisionTree_Day1.onnx',\n   'status': 'success'},\n  {'day': 'Day2', 'model': 'XGBoost', 'onnx_path': None, 'status': 'failed'},\n  {'day': 'Day2',\n   'model': 'CatBoost',\n   'onnx_path': 'onnx_models/CatBoost_Day2.onnx',\n   'status': 'success'},\n  {'day': 'Day2',\n   'model': 'AdaBoost',\n   'onnx_path': 'onnx_models/AdaBoost_Day2.onnx',\n   'status': 'success'},\n  {'day': 'Day2',\n   'model': 'RandomForest',\n   'onnx_path': 'onnx_models/RandomForest_Day2.onnx',\n   'status': 'success'},\n  {'day': 'Day2',\n   'model': 'DecisionTree',\n   'onnx_path': 'onnx_models/DecisionTree_Day2.onnx',\n   'status': 'success'},\n  {'day': 'Day3', 'model': 'XGBoost', 'onnx_path': None, 'status': 'failed'},\n  {'day': 'Day3',\n   'model': 'CatBoost',\n   'onnx_path': 'onnx_models/CatBoost_Day3.onnx',\n   'status': 'success'},\n  {'day': 'Day3',\n   'model': 'AdaBoost',\n   'onnx_path': 'onnx_models/AdaBoost_Day3.onnx',\n   'status': 'success'},\n  {'day': 'Day3',\n   'model': 'RandomForest',\n   'onnx_path': 'onnx_models/RandomForest_Day3.onnx',\n   'status': 'success'},\n  {'day': 'Day3',\n   'model': 'DecisionTree',\n   'onnx_path': 'onnx_models/DecisionTree_Day3.onnx',\n   'status': 'success'},\n  {'day': 'Day4', 'model': 'XGBoost', 'onnx_path': None, 'status': 'failed'},\n  {'day': 'Day4',\n   'model': 'CatBoost',\n   'onnx_path': 'onnx_models/CatBoost_Day4.onnx',\n   'status': 'success'},\n  {'day': 'Day4',\n   'model': 'AdaBoost',\n   'onnx_path': 'onnx_models/AdaBoost_Day4.onnx',\n   'status': 'success'},\n  {'day': 'Day4',\n   'model': 'RandomForest',\n   'onnx_path': 'onnx_models/RandomForest_Day4.onnx',\n   'status': 'success'},\n  {'day': 'Day4',\n   'model': 'DecisionTree',\n   'onnx_path': 'onnx_models/DecisionTree_Day4.onnx',\n   'status': 'success'}],\n      day         model                           onnx_path   status\n 0   Day0       XGBoost                                None   failed\n 1   Day0      CatBoost      onnx_models/CatBoost_Day0.onnx  success\n 2   Day0      AdaBoost      onnx_models/AdaBoost_Day0.onnx  success\n 3   Day0  RandomForest  onnx_models/RandomForest_Day0.onnx  success\n 4   Day0  DecisionTree  onnx_models/DecisionTree_Day0.onnx  success\n 5   Day1       XGBoost                                None   failed\n 6   Day1      CatBoost      onnx_models/CatBoost_Day1.onnx  success\n 7   Day1      AdaBoost      onnx_models/AdaBoost_Day1.onnx  success\n 8   Day1  RandomForest  onnx_models/RandomForest_Day1.onnx  success\n 9   Day1  DecisionTree  onnx_models/DecisionTree_Day1.onnx  success\n 10  Day2       XGBoost                                None   failed\n 11  Day2      CatBoost      onnx_models/CatBoost_Day2.onnx  success\n 12  Day2      AdaBoost      onnx_models/AdaBoost_Day2.onnx  success\n 13  Day2  RandomForest  onnx_models/RandomForest_Day2.onnx  success\n 14  Day2  DecisionTree  onnx_models/DecisionTree_Day2.onnx  success\n 15  Day3       XGBoost                                None   failed\n 16  Day3      CatBoost      onnx_models/CatBoost_Day3.onnx  success\n 17  Day3      AdaBoost      onnx_models/AdaBoost_Day3.onnx  success\n 18  Day3  RandomForest  onnx_models/RandomForest_Day3.onnx  success\n 19  Day3  DecisionTree  onnx_models/DecisionTree_Day3.onnx  success\n 20  Day4       XGBoost                                None   failed\n 21  Day4      CatBoost      onnx_models/CatBoost_Day4.onnx  success\n 22  Day4      AdaBoost      onnx_models/AdaBoost_Day4.onnx  success\n 23  Day4  RandomForest  onnx_models/RandomForest_Day4.onnx  success\n 24  Day4  DecisionTree  onnx_models/DecisionTree_Day4.onnx  success)"},"metadata":{}}],"source":"convert_all_models_from_notebook(X_train_multi, y_train_multi, X_test_multi, y_test_multi)"},{"block_group":"2532ddc5144444ddae226d9931fda55e","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"76c3c598e94c44769677bccf6cd6bc77","deepnote_block_group":"2532ddc5144444ddae226d9931fda55e","deepnote_cell_type":"markdown","deepnote_sorting_key":"16","deepnote_source":"NOTICE : The XGBoost can not run since the bug : could not convert string to float: '[2.7926846E1]'. We tried to fix that in the lib of XGBoost, we evenly change the line that need to debug but it still do not work. So,we continue to let the other models run and save in the folder."},"source":"NOTICE : The XGBoost can not run since the bug : could not convert string to float: '[2.7926846E1]'. We tried to fix that in the lib of XGBoost, we evenly change the line that need to debug but it still do not work. So,we continue to let the other models run and save in the folder."}],
        "metadata": {"deepnote_persisted_session":{"createdAt":"2025-11-17T16:17:27.278Z"},"deepnote_notebook_id":"076394a2f2b74bdc9159104ccaddd1c3"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }